{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba 基礎用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 Jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入繁體詞典\n",
    "jieba.set_dictionary(\"data/dict.txt.big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/fukuballlin/Projects/Hands-on-AI-Tools/data/dict.txt.big ...\n",
      "Loading model from cache /var/folders/ch/wch8w51d41d5j3yt9tc5tc240000gp/T/jieba.uf1926f5a6d4f9f4491d476b032371238.cache\n",
      "Loading model cost 0.784 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "塵世 / 中 / 一個 / 迷途 / 小 / 書僮\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"塵世中一個迷途小書僮\")\n",
    "print(\" / \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們 / 在 / 野生 / 動物園 / 玩\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我們在野生動物園玩\")\n",
    "print(\" / \".join(seg_list)) # 歧異詞辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林志傑 / 是 / 結巴 /   / PHP /   / 的 / 作者\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"林志傑是結巴 PHP 的作者\")\n",
    "print(\" / \".join(seg_list)) # 新詞辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "颱風 ns\n",
      "就是 d\n",
      "要 v\n",
      "泛舟 nz\n",
      "啊 zg\n",
      "不然 c\n",
      "要 v\n",
      "幹嘛 n\n"
     ]
    }
   ],
   "source": [
    "# 詞性標注\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"颱風就是要泛舟啊不然要幹嘛\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詞性列表請參考 https://gist.github.com/luw2007/6016931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 我沒有心\n",
      "我沒有真實的自我\n",
      "我只有消瘦的臉孔\n",
      "所謂軟弱\n",
      "所謂的順從一向是我\n",
      "的座右銘\n",
      "\n",
      "而我\n",
      "沒有那海洋的寬闊\n",
      "我只要熱情的撫摸\n",
      "所謂空洞\n",
      "所謂不安全感是我\n",
      "的墓誌銘\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "和我一般囉唆\n",
      "\n",
      "我沒有力\n",
      "我沒有滿腔的熱火\n",
      "我只有滿肚的如果\n",
      "所謂勇氣\n",
      "所謂的認同感是我\n",
      "隨便說說\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "是否對你來說\n",
      "只是一場遊戲\n",
      "雖然沒有把握\n",
      "\n",
      "而你\n",
      "是否和我一般退縮\n",
      "是否和我一般肌迫\n",
      "是否對你來說\n",
      "我 / 沒有 / 心 / \n",
      " / 我 / 沒有 / 真實 / 的 / 自我 / \n",
      " / 我 / 只有 / 消瘦 / 的 / 臉孔 / \n",
      " / 所謂 / 軟弱 / \n",
      " / 所謂 / 的 / 順從 / 一向 / 是 / 我 / \n",
      " / 的 / 座右銘 / \n",
      " / \n",
      " / 而 / 我 / \n",
      " / 沒有 / 那 / 海洋 / 的 / 寬闊 / \n",
      " / 我 / 只要 / 熱情 / 的 / 撫摸 / \n",
      " / 所謂 / 空洞 / \n",
      " / 所謂 / 不安全感 / 是 / 我 / \n",
      " / 的 / 墓誌銘 / \n",
      " / \n",
      " / 而 / 你 / \n",
      " / 是否 / 和 / 我 / 一般 / 怯懦 / \n",
      " / 是否 / 和 / 我 / 一般 / 矯作 / \n",
      " / 和 / 我 / 一般 / 囉唆 / \n",
      " / \n",
      " / 我 / 沒有 / 力 / \n",
      " / 我 / 沒有 / 滿腔 / 的 / 熱火 / \n",
      " / 我 / 只有 / 滿肚 / 的 / 如果 / \n",
      " / 所謂 / 勇氣 / \n",
      " / 所謂 / 的 / 認同感 / 是 / 我 / \n",
      " / 隨便說說 / \n",
      " / \n",
      " / 而 / 你 / \n",
      " / 是否 / 和 / 我 / 一般 / 怯懦 / \n",
      " / 是否 / 和 / 我 / 一般 / 矯作 / \n",
      " / 是否 / 對 / 你 / 來說 / \n",
      " / 只是 / 一場 / 遊戲 / \n",
      " / 雖然 / 沒有 / 把握 / \n",
      " / \n",
      " / 而 / 你 / \n",
      " / 是否 / 和 / 我 / 一般 / 退縮 / \n",
      " / 是否 / 和 / 我 / 一般 / 肌迫 / \n",
      " / 是否 / 對 / 你 / 來說\n"
     ]
    }
   ],
   "source": [
    "# 實例：中文歌詞斷詞\n",
    "content = open('data/lyric1.txt', 'rb').read()\n",
    "print \"Input：\", content\n",
    "words = jieba.cut(content)\n",
    "print(\" / \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 我沒有心\n",
      "我沒有真實的自我\n",
      "我只有消瘦的臉孔\n",
      "所謂軟弱\n",
      "所謂的順從一向是我\n",
      "的座右銘\n",
      "\n",
      "而我\n",
      "沒有那海洋的寬闊\n",
      "我只要熱情的撫摸\n",
      "所謂空洞\n",
      "所謂不安全感是我\n",
      "的墓誌銘\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "和我一般囉唆\n",
      "\n",
      "我沒有力\n",
      "我沒有滿腔的熱火\n",
      "我只有滿肚的如果\n",
      "所謂勇氣\n",
      "所謂的認同感是我\n",
      "隨便說說\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "是否對你來說\n",
      "只是一場遊戲\n",
      "雖然沒有把握\n",
      "\n",
      "而你\n",
      "是否和我一般退縮\n",
      "是否和我一般肌迫\n",
      "是否對你來說\n",
      "Output：\n",
      "所謂 / 沒有 / 是否 / 一般 / 矯作 / 來說 / 怯懦 / 墓誌銘 / 退縮 / 寬闊\n"
     ]
    }
   ],
   "source": [
    "# 實例：取出文章中的關鍵詞\n",
    "import jieba.analyse\n",
    "content = open('data/lyric1.txt', 'rb').read()\n",
    "print \"Input：\", content\n",
    "tags = jieba.analyse.extract_tags(content, 10)\n",
    "print \"Output：\"\n",
    "print(\" / \".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 我沒有心\n",
      "我沒有真實的自我\n",
      "我只有消瘦的臉孔\n",
      "所謂軟弱\n",
      "所謂的順從一向是我\n",
      "的座右銘\n",
      "\n",
      "而我\n",
      "沒有那海洋的寬闊\n",
      "我只要熱情的撫摸\n",
      "所謂空洞\n",
      "所謂不安全感是我\n",
      "的墓誌銘\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "和我一般囉唆\n",
      "\n",
      "我沒有力\n",
      "我沒有滿腔的熱火\n",
      "我只有滿肚的如果\n",
      "所謂勇氣\n",
      "所謂的認同感是我\n",
      "隨便說說\n",
      "\n",
      "而你\n",
      "是否和我一般怯懦\n",
      "是否和我一般矯作\n",
      "是否對你來說\n",
      "只是一場遊戲\n",
      "雖然沒有把握\n",
      "\n",
      "而你\n",
      "是否和我一般退縮\n",
      "是否和我一般肌迫\n",
      "是否對你來說\n",
      "Output：\n",
      "所謂 / 是否 / 一般 / 矯作 / 來說 / 怯懦 / 墓誌銘 / 退縮 / 寬闊 / 順從\n"
     ]
    }
   ],
   "source": [
    "# 實例：關鍵詞去除停用字\n",
    "jieba.analyse.set_stop_words(\"data/stop_words.txt\")\n",
    "content = open('data/lyric1.txt', 'rb').read()\n",
    "print \"Input：\", content\n",
    "tags = jieba.analyse.extract_tags(content, 10)\n",
    "print \"Output：\"\n",
    "print(\" / \".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 親愛的媽媽\n",
      "請你毋通煩惱我\n",
      "原諒我\n",
      "行袂開跤\n",
      "我欲去對抗袂當原諒的人\n",
      "\n",
      "歹勢啦\n",
      "愛人啊\n",
      "袂當陪你去看電影\n",
      "原諒我\n",
      "行袂開跤\n",
      "我欲去對抗欺負咱的人\n",
      "\n",
      "天色漸漸光\n",
      "遮有一陣人\n",
      "為了守護咱的夢\n",
      "成做更加勇敢的人\n",
      "\n",
      "已經袂記\n",
      "是第幾工\n",
      "請毋通煩惱我\n",
      "因為阮知道\n",
      "無行過寒冬\n",
      "袂有花開的一工\n",
      "\n",
      "天色漸漸光\n",
      "天色漸漸光\n",
      "已經是更加勇敢的人\n",
      "\n",
      "天色漸漸光\n",
      "咱就大聲來唱著歌\n",
      "一直到希望的光線\n",
      "照光島嶼每一個人\n",
      "\n",
      "天色漸漸光\n",
      "咱就大聲來唱著歌\n",
      "日頭一爬上山\n",
      "親愛 / 的 / 媽媽 / \n",
      " / 請 / 你 / 毋通 / 煩惱 / 我 / \n",
      " / 原諒 / 我 / \n",
      " / 行袂 / 開跤 / \n",
      " / 我 / 欲 / 去 / 對抗 / 袂 / 當 / 原諒 / 的 / 人 / \n",
      " / \n",
      " / 歹勢 / 啦 / \n",
      " / 愛人 / 啊 / \n",
      " / 袂 / 當 / 陪你去 / 看 / 電影 / \n",
      " / 原諒 / 我 / \n",
      " / 行袂 / 開跤 / \n",
      " / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 遮有 / 一陣 / 人 / \n",
      " / 為 / 了 / 守護 / 咱 / 的 / 夢 / \n",
      " / 成 / 做 / 更加 / 勇敢的人 / \n",
      " / \n",
      " / 已經 / 袂 / 記 / \n",
      " / 是 / 第幾 / 工 / \n",
      " / 請 / 毋通 / 煩惱 / 我 / \n",
      " / 因為 / 阮 / 知道 / \n",
      " / 無行過 / 寒冬 / \n",
      " / 袂 / 有 / 花開 / 的 / 一工 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 已經 / 是 / 更加 / 勇敢的人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / \n",
      " / 一直 / 到 / 希望 / 的 / 光線 / \n",
      " / 照光 / 島嶼 / 每 / 一個 / 人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / \n",
      " / 日頭 / 一爬 / 上山\n"
     ]
    }
   ],
   "source": [
    "# 實例：台語歌詞斷詞\n",
    "content = open('data/lyric2.txt', 'rb').read()\n",
    "print \"Input：\", content\n",
    "words = jieba.cut(content)\n",
    "print(\" / \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 親愛的媽媽\n",
      "請你毋通煩惱我\n",
      "原諒我\n",
      "行袂開跤\n",
      "我欲去對抗袂當原諒的人\n",
      "\n",
      "歹勢啦\n",
      "愛人啊\n",
      "袂當陪你去看電影\n",
      "原諒我\n",
      "行袂開跤\n",
      "我欲去對抗欺負咱的人\n",
      "\n",
      "天色漸漸光\n",
      "遮有一陣人\n",
      "為了守護咱的夢\n",
      "成做更加勇敢的人\n",
      "\n",
      "已經袂記\n",
      "是第幾工\n",
      "請毋通煩惱我\n",
      "因為阮知道\n",
      "無行過寒冬\n",
      "袂有花開的一工\n",
      "\n",
      "天色漸漸光\n",
      "天色漸漸光\n",
      "已經是更加勇敢的人\n",
      "\n",
      "天色漸漸光\n",
      "咱就大聲來唱著歌\n",
      "一直到希望的光線\n",
      "照光島嶼每一個人\n",
      "\n",
      "天色漸漸光\n",
      "咱就大聲來唱著歌\n",
      "日頭一爬上山\n",
      "親愛 / 的 / 媽媽 / \n",
      " / 請 / 你 / 毋通 / 煩惱 / 我 / \n",
      " / 原諒 / 我 / \n",
      " / 行袂開跤 / \n",
      " / 我 / 欲 / 去 / 對抗 / 袂當 / 原諒 / 的 / 人 / \n",
      " / \n",
      " / 歹勢 / 啦 / \n",
      " / 愛人 / 啊 / \n",
      " / 袂當 / 陪你去 / 看 / 電影 / \n",
      " / 原諒 / 我 / \n",
      " / 行袂開跤 / \n",
      " / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 遮有 / 一陣 / 人 / \n",
      " / 為 / 了 / 守護 / 咱 / 的 / 夢 / \n",
      " / 成 / 做 / 更加 / 勇敢的人 / \n",
      " / \n",
      " / 已經 / 袂記 / \n",
      " / 是 / 第幾 / 工 / \n",
      " / 請 / 毋通 / 煩惱 / 我 / \n",
      " / 因為 / 阮 / 知道 / \n",
      " / 無行過 / 寒冬 / \n",
      " / 袂有 / 花開 / 的 / 一工 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 已經 / 是 / 更加 / 勇敢的人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / \n",
      " / 一直 / 到 / 希望 / 的 / 光線 / \n",
      " / 照光 / 島嶼 / 每 / 一個 / 人 / \n",
      " / \n",
      " / 天色 / 漸漸 / 光 / \n",
      " / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / \n",
      " / 日頭 / 一爬 / 上山\n"
     ]
    }
   ],
   "source": [
    "# 實例：台語歌詞斷詞，使用自定義詞庫\n",
    "jieba.load_userdict(\"data/userdict.txt\")\n",
    "content = open('data/lyric2.txt', 'rb').read()\n",
    "print \"Input：\", content\n",
    "words = jieba.cut(content)\n",
    "print(\" / \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA 潛在語意分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1：中文斷詞，去掉停用字，集合成資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 / 是 / 我 / 的 / 巧克力 / ， / 你 / 是 / 這 / 世界 / 最美 / 的 / 風景 / \n",
      "\n",
      "愛情 / 的 / 甜美 / ， / 添加 / 了 / 你 / 我 / 的 / 理想 / ， / 如 / 巧克力 / 般的 / 快樂 / 滋味 / \n",
      "\n",
      "你 / 我 / 的 / 世界 / 中 / ， / 愛情 / 的 / 咖啡 / 那麼 / 甜美 / \n",
      "\n",
      "愛情 / 如 / 風景 / 般美 / ， / 愛情 / 如 / 咖啡 / 般濃 / \n",
      "\n",
      "愛情 / 的 / 滋味 / 很 / 甜美 / \n",
      "\n",
      "我要 / 達成 / 夢想 / \n",
      "\n",
      "抓 / 一把 / 陽光 / ， / 夢想 / 開始 / \n",
      "\n",
      "等待 / 陽光 / ， / 往前 / 飛翔 / ， / 向 / 夢想 / \n",
      "\n",
      "擁抱 / 陽光 / ， / 滿懷 / 理想 / ， / 到處 / 飛翔\n",
      "[[u'\\u5de7\\u514b\\u529b', u'\\u4e16\\u754c', u'\\u98a8\\u666f'], [u'\\u611b\\u60c5', u'\\u751c\\u7f8e', u'\\u7406\\u60f3', u'\\u5de7\\u514b\\u529b', u'\\u5feb\\u6a02', u'\\u6ecb\\u5473'], [u'\\u4e16\\u754c', u'\\u611b\\u60c5', u'\\u5496\\u5561', u'\\u751c\\u7f8e'], [u'\\u611b\\u60c5', u'\\u98a8\\u666f', u'\\u611b\\u60c5', u'\\u5496\\u5561'], [u'\\u611b\\u60c5', u'\\u6ecb\\u5473', u'\\u751c\\u7f8e'], [u'\\u5922\\u60f3'], [u'\\u967d\\u5149', u'\\u5922\\u60f3'], [u'\\u967d\\u5149', u'\\u98db\\u7fd4', u'\\u5922\\u60f3'], [u'\\u967d\\u5149', u'\\u7406\\u60f3', u'\\u98db\\u7fd4']]\n"
     ]
    }
   ],
   "source": [
    "stop_words = [u'，', u'你', u'是', u'我', u'的', u'這', u'最美', u'的', u'添加',  u'了', u'如', u'般的', u'中', u'那麼', u'般美', u'般濃', u'很', u'我要', u'達成', u'抓', u'一把', u'開始', u'等待', u'往前', u'向', u'擁抱', u'滿懷', u'到處']\n",
    "lyrics_dataset = []\n",
    "with open('data/demo_lyrics.txt') as fp:\n",
    "    for line in fp:\n",
    "        words = jieba.cut(line)\n",
    "        seg_sentence = \" / \".join(words)\n",
    "        print(seg_sentence)\n",
    "        words = seg_sentence.split(\" / \")\n",
    "        seg_remove_stop_words = []\n",
    "        for word in words:\n",
    "            if (word not in stop_words):\n",
    "                if word != u'\\n':\n",
    "                    seg_remove_stop_words.append(word)\n",
    "        lyrics_dataset.append(seg_remove_stop_words)\n",
    "print(lyrics_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u5de7\\u514b\\u529b', u'\\u4e16\\u754c', u'\\u98a8\\u666f', u'\\u611b\\u60c5', u'\\u751c\\u7f8e', u'\\u7406\\u60f3', u'\\u5feb\\u6a02', u'\\u6ecb\\u5473', u'\\u5496\\u5561', u'\\u5922\\u60f3', u'\\u967d\\u5149', u'\\u98db\\u7fd4']\n"
     ]
    }
   ],
   "source": [
    "lyrics_dic = []\n",
    "for seg_remove_stop_words in lyrics_dataset:\n",
    "    for word in seg_remove_stop_words:\n",
    "        if word not in lyrics_dic:\n",
    "            lyrics_dic.append(word)\n",
    "print(lyrics_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巧克力\n",
      "世界\n",
      "風景\n",
      "愛情\n",
      "甜美\n",
      "理想\n",
      "快樂\n",
      "滋味\n",
      "咖啡\n",
      "夢想\n",
      "陽光\n",
      "飛翔\n"
     ]
    }
   ],
   "source": [
    "for word in lyrics_dic:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2：將每首歌詞轉成向量表示(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "[[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "lyrics_dataset_vec = []\n",
    "for seg_remove_stop_words in lyrics_dataset:\n",
    "    seg_remove_stop_words_vec = []\n",
    "    for word in lyrics_dic:\n",
    "        count = 0\n",
    "        for word2 in seg_remove_stop_words:\n",
    "            if word == word2: \n",
    "                count = count+1\n",
    "        seg_remove_stop_words_vec.append(count)\n",
    "    print(seg_remove_stop_words_vec)\n",
    "    lyrics_dataset_vec.append(seg_remove_stop_words_vec)\n",
    "print(lyrics_dataset_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3：LSA 語意分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 1 1 1 1 0 0 0 0]\n",
      " [0 1 0 1 1 0 0 0 1 0 0 0]\n",
      " [0 0 1 2 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lyrics_dataset_vec = np.array(lyrics_dataset_vec)\n",
    "print(lyrics_dataset_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11867629  0.10716535  0.1248784   0.42673089  0.23499908  0.08663343\n",
      "   0.0896638   0.15684622  0.17401877 -0.0284894  -0.02673936 -0.01668155]\n",
      " [ 0.40807287  0.32862817  0.37684866  1.34833923  0.77396951  0.42029799\n",
      "   0.31840907  0.53500513  0.52614924  0.17497701  0.25081549  0.18944092]\n",
      " [ 0.31711724  0.28883819  0.33695866  1.14767727  0.63006568  0.22388123\n",
      "   0.23896438  0.41938035  0.46949113 -0.09310391 -0.09277046 -0.05992616]\n",
      " [ 0.38305077  0.3546717   0.41463648  1.4035487   0.76600755  0.25268271\n",
      "   0.28718486  0.50720176  0.57757637 -0.15203337 -0.1617535  -0.1081679 ]\n",
      " [ 0.28377848  0.24759839  0.28719932  0.99456291  0.55452912  0.23373414\n",
      "   0.21659606  0.37411315  0.40043287 -0.00886385  0.01048022  0.01369606]\n",
      " [ 0.02127003 -0.020197   -0.02956243 -0.04113617  0.00842787  0.13652039\n",
      "   0.02605044  0.02384445 -0.04019861  0.26468348  0.33401241  0.24096356]\n",
      " [ 0.05131676 -0.04290211 -0.06364341 -0.08185678  0.02531442  0.31148481\n",
      "   0.06137457  0.05815872 -0.0864299   0.59869589  0.75575728  0.54528828]\n",
      " [ 0.07390083 -0.0584942  -0.08731693 -0.10806453  0.03926697  0.43846817\n",
      "   0.08755201  0.08410999 -0.11850877  0.83965945  1.06008201  0.76490397]\n",
      " [ 0.09885855 -0.01811351 -0.03753252  0.03493956  0.10394385  0.40189653\n",
      "   0.10188892  0.119027   -0.04958529  0.71149636  0.90103401  0.65092377]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg, diag, dot\n",
    "num_dimensions = 2\n",
    "u, s, vt = linalg.svd(lyrics_dataset_vec)\n",
    "u = u[:, :num_dimensions]\n",
    "sigma = diag(s)[:num_dimensions, :num_dimensions]\n",
    "vt = vt[:num_dimensions, :]\n",
    "low_rank_document_term_matrix = dot(u, dot(sigma, vt))\n",
    "print(low_rank_document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4：使用降維後的向量計算 cosin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11867629  0.10716535  0.1248784   0.42673089  0.23499908  0.08663343\n",
      "  0.0896638   0.15684622  0.17401877 -0.0284894  -0.02673936 -0.01668155]\n"
     ]
    }
   ],
   "source": [
    "low_rank_document_term_vec = low_rank_document_term_matrix[0]\n",
    "print(low_rank_document_term_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.96405920759\n",
      "0.999795153363\n",
      "0.998286296793\n",
      "0.996703555514\n",
      "-0.093506010014\n",
      "-0.0801314382428\n",
      "-0.0743859837781\n",
      "0.0522015275412\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "for vec in low_rank_document_term_matrix:\n",
    "    score = 1 - spatial.distance.cosine(low_rank_document_term_vec, vec)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.093506010014\n",
      "0.17437811112\n",
      "-0.11363802377\n",
      "-0.151608348366\n",
      "-0.0124235605267\n",
      "1.0\n",
      "0.999909881114\n",
      "0.999815914944\n",
      "0.989380103024\n"
     ]
    }
   ],
   "source": [
    "low_rank_document_term_vec = low_rank_document_term_matrix[5]\n",
    "for vec in low_rank_document_term_matrix:\n",
    "    score = 1 - spatial.distance.cosine(low_rank_document_term_vec, vec)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16841129  0.02549759]\n"
     ]
    }
   ],
   "source": [
    "query_vector = lyrics_dataset_vec[0, :]\n",
    "low_dimensional_query = dot(linalg.inv(sigma), dot(vt, query_vector))\n",
    "print(low_dimensional_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11867629  0.10716535  0.1248784   0.42673089  0.23499908  0.08663343\n",
      "  0.0896638   0.15684622  0.17401877 -0.0284894  -0.02673936 -0.01668155]\n"
     ]
    }
   ],
   "source": [
    "lsa_query = dot(vt.T, dot(sigma, low_dimensional_query.T))\n",
    "print(lsa_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.96405920759\n",
      "0.999795153363\n",
      "0.998286296793\n",
      "0.996703555514\n",
      "-0.093506010014\n",
      "-0.0801314382428\n",
      "-0.0743859837781\n",
      "0.0522015275412\n"
     ]
    }
   ],
   "source": [
    "for vec in low_rank_document_term_matrix:\n",
    "    score = 1 - spatial.distance.cosine(lsa_query, vec)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
